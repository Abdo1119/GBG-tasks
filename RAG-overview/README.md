# RAG Overview â€” Ø§Ù„Ù‚Ù„Ø¹Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ ğŸ°

A minimal **Retrieval-Augmented Generation (RAG)** pipeline over Arabic text, built as a learning project.

## What It Does

You ask a question in Arabic about "Ø§Ù„Ù‚Ù„Ø¹Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡" (The White Castle) â†’ the system finds the most relevant text passages â†’ Gemini generates a smart answer based on those passages.

## Architecture

```
arabic.txt â†’ Clean â†’ Sentence Split â†’ 7 Chunks (400-800 chars, 1-sentence overlap)
                                          â†“
                                   BGE-M3 Encode (embeddings)
                                          â†“
                                   ChromaDB (persistent vector store)
                                          â†“
                          User Query â†’ BGE-M3 Encode â†’ ChromaDB Search
                                          â†“
                                   Top-K Relevant Chunks
                                          â†“
                              Chunks + Query â†’ Gemini 2.0 Flash
                                          â†“
                                   Arabic Answer + Citations
                                          â†“
                                   Streamlit Web UI (RTL)
```

## Tech Stack

| Component | Tool |
|-----------|------|
| Embedding Model | `BAAI/bge-m3` (multilingual, strong on Arabic) |
| Vector Database | ChromaDB (persistent, local) |
| LLM | Gemini 2.0 Flash (via Google AI API) |
| Web UI | Streamlit (RTL support) |
| Language | Python 3.10+ |

## Pipeline Steps

### 1. Text Loading & Cleaning
- Loads `arabic.txt` (UTF-8)
- Normalizes whitespace, removes extra blank lines

### 2. Chunking
- Splits text into sentences using Arabic punctuation (`.` `ØŸ` `!` `Ø›`)
- Groups sentences into chunks of 400â€“800 characters
- 1-sentence overlap between consecutive chunks to preserve context
- Result: **7 chunks** from 21 sentences

### 3. Embedding & Indexing
- Each chunk is encoded into a 1024-dim vector using `BAAI/bge-m3`
- Vectors are normalized (`normalize_embeddings=True`) so dot product = cosine similarity
- Stored in **ChromaDB** (persistent on disk in `./chroma_db/`)
- File hash tracking: re-encodes only when `arabic.txt` changes

### 4. Retrieval
- User query is encoded with the same model
- ChromaDB finds the closest chunks by vector similarity
- Returns top-k results with similarity scores

### 5. Generation
- Retrieved chunks + user query are sent to **Gemini 2.0 Flash**
- Prompt instructs Gemini to answer in Arabic using only the provided chunks
- `max_output_tokens=500`, `temperature=0.3` for concise, focused answers

### 6. Web Interface
- Streamlit app with full RTL (right-to-left) support
- Color-coded results: ğŸŸ¢ high similarity | ğŸŸ¡ medium | ğŸ”´ low
- Sidebar with system info and example questions

## Setup & Run

```bash
# Install dependencies
pip install -r requirements.txt

# Run the app
streamlit run rag_arabic.py
```

> First run downloads the BGE-M3 model (~2GB). After that, embeddings are cached in ChromaDB.

## Files

```
RAG-overview/
â”œâ”€â”€ rag_arabic.py      # Main RAG pipeline + Streamlit UI
â”œâ”€â”€ arabic.txt          # Source text (Arabic, about Ø§Ù„Ù‚Ù„Ø¹Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡)
â”œâ”€â”€ english.txt         # English version of the text
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # This file
```

## Example Questions

- Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ ÙÙŠ Ø§Ù„Ù‚Ù„Ø¹Ø©ØŸ
- Ù…Ø§Ø°Ø§ Ø­Ø¯Ø« ÙÙŠ Ø§Ù„Ø¹Ù‡Ø¯ Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØŸ
- Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø³Ø±ÙŠØ©ØŸ
- Ù…ØªÙ‰ ØªÙ… ØªØ±Ù…ÙŠÙ… Ø§Ù„Ù‚Ù„Ø¹Ø©ØŸ

"""
Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø¹Ø²Ø² Ø¨Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (RAG) Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
BAAI/bge-m3 + ChromaDB + Gemini + Streamlit

Usage:
    streamlit run rag_arabic.py
"""

import sys
import re
import os
import hashlib

sys.stdout.reconfigure(encoding="utf-8", errors="replace")
sys.stderr.reconfigure(encoding="utf-8", errors="replace")

import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai

# â”€â”€â”€ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TEXT_FILE = "arabic.txt"
MODEL_NAME = "BAAI/bge-m3"
CHROMA_DIR = "./chroma_db"
COLLECTION_NAME = "arabic_rag"
CHUNK_MIN_CHARS = 400
CHUNK_MAX_CHARS = 800
OVERLAP_SENTENCES = 1

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "AIzaSyAtV7r2_u0VzLvjC8KHq_wkTD_ohlDZ0gg")
GEMINI_MODEL = "gemini-2.0-flash"


# â”€â”€â”€ ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_text(path: str) -> str:
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù†ØµÙŠ Ø¨ØªØ±Ù…ÙŠØ² UTF-8."""
    if not os.path.isfile(path):
        st.error(f"Ø§Ù„Ù…Ù„Ù '{path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        st.stop()
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def clean_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ."""
    text = text.strip()
    text = re.sub(r"[^\S\n]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text


def file_hash(path: str) -> str:
    """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù."""
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()


# â”€â”€â”€ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _split_sentences(text: str) -> list[str]:
    parts = re.split(r"(?<=[.Û”ã€‚ØŸ?!Ø›\n])\s*", text)
    return [s.strip() for s in parts if s.strip()]


def split_into_chunks(
    text: str,
    min_chars: int = CHUNK_MIN_CHARS,
    max_chars: int = CHUNK_MAX_CHARS,
    overlap: int = OVERLAP_SENTENCES,
) -> list[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ Ù…ØªØ¯Ø§Ø®Ù„Ø©."""
    sentences = _split_sentences(text)
    if not sentences:
        return []

    result_chunks: list[str] = []
    i = 0

    while i < len(sentences):
        current_chunk_sents: list[str] = []
        current_len = 0

        while i < len(sentences):
            sent = sentences[i]
            new_len = current_len + len(sent) + (1 if current_chunk_sents else 0)

            if new_len > max_chars and current_len >= min_chars:
                break

            current_chunk_sents.append(sent)
            current_len = new_len
            i += 1

            if current_len >= min_chars and i < len(sentences):
                next_len = current_len + len(sentences[i]) + 1
                if next_len > max_chars:
                    break

        if current_chunk_sents:
            result_chunks.append(" ".join(current_chunk_sents))

        if i < len(sentences) and overlap > 0:
            i -= min(overlap, len(current_chunk_sents) - 1)

    return result_chunks


# â”€â”€â”€ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ + ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def encode(model: SentenceTransformer, texts: list[str]) -> list[list[float]]:
    """ØªØ­ÙˆÙŠÙ„ Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡Ø§Øª."""
    emb = model.encode(texts, normalize_embeddings=True, show_progress_bar=False)
    return emb.tolist()


def init_chroma() -> chromadb.ClientAPI:
    return chromadb.PersistentClient(path=CHROMA_DIR)


def build_index(
    client: chromadb.ClientAPI,
    model: SentenceTransformer,
    text_chunks: list[str],
    text_hash: str,
) -> chromadb.Collection:
    """Ø¨Ù†Ø§Ø¡ Ø£Ùˆ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙÙ‡Ø±Ø³ ChromaDB."""
    existing = [c.name for c in client.list_collections()]

    if COLLECTION_NAME in existing:
        coll = client.get_collection(COLLECTION_NAME)
        stored_meta = coll.metadata or {}
        if stored_meta.get("file_hash") == text_hash:
            return coll
        else:
            client.delete_collection(COLLECTION_NAME)

    coll = client.create_collection(
        name=COLLECTION_NAME,
        metadata={"file_hash": text_hash},
    )

    embeddings = encode(model, text_chunks)
    ids = [f"chunk_{i}" for i in range(len(text_chunks))]
    metadatas = [{"chunk_id": i, "char_len": len(c)} for i, c in enumerate(text_chunks)]

    coll.add(ids=ids, embeddings=embeddings, documents=text_chunks, metadatas=metadatas)
    return coll


# â”€â”€â”€ Ø§Ù„Ø¨Ø­Ø« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search(
    model: SentenceTransformer,
    collection: chromadb.Collection,
    query: str,
    top_k: int = 5,
) -> list[dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ChromaDB."""
    query_emb = model.encode([query], normalize_embeddings=True).tolist()

    results = collection.query(
        query_embeddings=query_emb,
        n_results=min(top_k, collection.count()),
        include=["documents", "metadatas", "distances"],
    )

    output = []
    for i in range(len(results["ids"][0])):
        distance = results["distances"][0][i]
        score = 1.0 - distance

        output.append({
            "chunk_id": results["metadatas"][0][i]["chunk_id"],
            "score": score,
            "text": results["documents"][0][i],
        })
    return output


# â”€â”€â”€ Gemini Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ask_gemini(query: str, retrieved_chunks: list[dict]) -> str:
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ø¥Ù„Ù‰ Gemini Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©."""
    context = ""
    for i, chunk in enumerate(retrieved_chunks, start=1):
        context += f"[Ù…Ù‚Ø·Ø¹ {i}]: {chunk['text']}\n\n"

    prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙ‚Ø· Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ù„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Øµ.
Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ØŒ Ù‚Ù„ "Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ§Ø­".

Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø®ØªØµØ±Ø©ØŒ Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©."""

    genai.configure(api_key=GEMINI_API_KEY)
    gemini = genai.GenerativeModel(
        GEMINI_MODEL,
        generation_config=genai.GenerationConfig(
            max_output_tokens=500,
            temperature=0.3,
        ),
    )
    response = gemini.generate_content(prompt)
    return response.text


# â”€â”€â”€ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ (cached) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_resource(show_spinner="Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
def load_model():
    return SentenceTransformer(MODEL_NAME)


@st.cache_resource(show_spinner="Ø¬Ø§Ø±Ù Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³...")
def load_index():
    model = load_model()
    raw = load_text(TEXT_FILE)
    text = clean_text(raw)
    text_h = file_hash(TEXT_FILE)
    chunks = split_into_chunks(text)
    client = init_chroma()
    coll = build_index(client, model, chunks, text_h)
    return model, coll, chunks


# â”€â”€â”€ ÙˆØ§Ø¬Ù‡Ø© Streamlit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    st.set_page_config(
        page_title="RAG â€” Ø§Ù„Ù‚Ù„Ø¹Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡",
        page_icon="ğŸ°",
        layout="centered",
    )

    # RTL support
    st.markdown("""
    <style>
        .stApp { direction: rtl; }
        .stTextInput input, .stTextArea textarea { direction: rtl; text-align: right; }
        .stMarkdown { direction: rtl; text-align: right; }
        .result-box {
            border-right: 4px solid;
            padding: 12px 16px;
            margin: 8px 0;
            border-radius: 8px;
            line-height: 1.8;
        }
        .score-high { border-color: #22c55e; background: rgba(34,197,94,0.08); }
        .score-mid  { border-color: #eab308; background: rgba(234,179,8,0.08); }
        .score-low  { border-color: #ef4444; background: rgba(239,68,68,0.08); }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("<h1 style='text-align:center;'>ğŸ° Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ø²Ø² â€” Ø§Ù„Ù‚Ù„Ø¹Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align:center; color:gray;'>BAAI/bge-m3 + ChromaDB + Gemini</p>", unsafe_allow_html=True)

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ÙÙ‡Ø±Ø³
    model, collection, chunks = load_index()

    st.markdown("---")

    # Ø®Ø§Ù†Ø© Ø§Ù„Ø³Ø¤Ø§Ù„
    query = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø§Ù„Ù‚Ù„Ø¹Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡:", placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ ÙÙŠ Ø§Ù„Ù‚Ù„Ø¹Ø©ØŸ")

    col1, col2 = st.columns([3, 1])
    with col2:
        top_k = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", 1, 7, 3)

    if query:
        # Ø§Ù„Ø¨Ø­Ø«
        with st.spinner("Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø«..."):
            results = search(model, collection, query, top_k=top_k)

        # Ø¥Ø¬Ø§Ø¨Ø© Gemini
        with st.spinner("Ø¬Ø§Ø±Ù ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Gemini..."):
            try:
                answer = ask_gemini(query, results)
                st.markdown("### ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©")
                st.markdown(f"<div style='background:#1a1a2e; padding:16px; border-radius:10px; line-height:2;'>{answer}</div>", unsafe_allow_html=True)
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ Gemini: {e}")

        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©
        st.markdown("### ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©")

        for i, r in enumerate(results, start=1):
            score = r["score"]
            if score >= 0.5:
                css_class = "score-high"
            elif score >= 0.3:
                css_class = "score-mid"
            else:
                css_class = "score-low"

            st.markdown(
                f"""<div class="result-box {css_class}">
                <strong>Ø§Ù„Ù…Ù‚Ø·Ø¹ {i}</strong> â€” Ø±Ù‚Ù…: {r['chunk_id']} | Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {score:.4f}<br><br>
                {r['text']}
                </div>""",
                unsafe_allow_html=True,
            )

    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    with st.sidebar:
        st.markdown("### Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        st.markdown(f"- **Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹:** {len(chunks)}")
        st.markdown(f"- **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:** `{MODEL_NAME}`")
        st.markdown(f"- **LLM:** `{GEMINI_MODEL}`")
        st.markdown(f"- **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** ChromaDB")
        st.markdown("---")
        st.markdown("**Ø£Ù…Ø«Ù„Ø© Ø£Ø³Ø¦Ù„Ø©:**")
        st.markdown("- Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆÙ„ÙŠÙƒÙŠ ÙÙŠ Ø§Ù„Ù‚Ù„Ø¹Ø©ØŸ")
        st.markdown("- Ù…Ø§Ø°Ø§ Ø­Ø¯Ø« ÙÙŠ Ø§Ù„Ø¹Ù‡Ø¯ Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØŸ")
        st.markdown("- Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø³Ø±ÙŠØ©ØŸ")
        st.markdown("- Ù…ØªÙ‰ ØªÙ… ØªØ±Ù…ÙŠÙ… Ø§Ù„Ù‚Ù„Ø¹Ø©ØŸ")


if __name__ == "__main__":
    main()
